<!DOCTYPE html><html lang="en-us"><head> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-61575797-6"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-61575797-6'); </script><link href="http://gmpg.org/xfn/11" rel="profile"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1"><title> Talking to myself using GPT &middot; tensor.blog</title><link rel="canonical" href="http://localhost:4000/2021/07/14/jaegpt/"><style> *{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box}html,body{margin:0;padding:0}html{font-family:"Helvetica Neue", Helvetica, Arial, sans-serif;font-size:16px;line-height:1.5}@media (min-width: 38em){html{font-size:20px}}body{color:#515151;background-color:#fff;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%}a{color:#268bd2;text-decoration:none}a strong{color:inherit}a:hover,a:focus{text-decoration:underline}h1,h2,h3,h4,h5,h6{margin-bottom:.5rem;font-weight:bold;line-height:1.25;color:#313131;text-rendering:optimizeLegibility}h1{font-size:2rem}h2{margin-top:1rem;font-size:1.5rem}h3{margin-top:1.5rem;font-size:1.25rem}h4,h5,h6{margin-top:1rem;font-size:1rem}p{margin-top:0;margin-bottom:1rem}strong{color:#303030}ul,ol,dl{margin-top:0;margin-bottom:1rem}dt{font-weight:bold}dd{margin-bottom:.5rem}hr{position:relative;margin:1.5rem 0;border:0;border-top:1px solid #eee;border-bottom:1px solid #fff}abbr{font-size:85%;font-weight:bold;color:#555;text-transform:uppercase}abbr[title]{cursor:help;border-bottom:1px dotted #e5e5e5}code,pre{font-family:Menlo, Monaco, "Courier New", monospace}code{padding:.25em .5em;font-size:85%;color:#bf616a;background-color:#f9f9f9;border-radius:3px}pre{display:block;margin-top:0;margin-bottom:1rem;padding:1rem;font-size:.8rem;line-height:1.4;white-space:pre;white-space:pre-wrap;word-break:break-all;word-wrap:break-word;background-color:#f9f9f9}pre code{padding:0;font-size:100%;color:inherit;background-color:transparent}.highlight{margin-bottom:1rem;border-radius:4px}.highlight pre{margin-bottom:0}.gist .gist-file{font-family:Menlo, Monaco, "Courier New", monospace !important}.gist .markdown-body{padding:15px}.gist pre{padding:0;background-color:transparent}.gist .gist-file .gist-data{font-size:.8rem !important;line-height:1.4}.gist code{padding:0;color:inherit;background-color:transparent;border-radius:0}blockquote{padding:.5rem 1rem;margin:.8rem 0;color:#7a7a7a;border-left:.25rem solid #e5e5e5}blockquote p:last-child{margin-bottom:0}@media (min-width: 30em){blockquote{padding-right:5rem;padding-left:1.25rem}}img{display:block;max-width:100%;margin:0 auto 1rem auto;border-radius:5px}img.inline{display:inline;margin:0;border-radius:0;position:relative;top:2px}table{margin-bottom:1rem;width:100%;border:1px solid #e5e5e5;border-collapse:collapse}td,th{padding:.25rem .5rem;border:1px solid #e5e5e5}tbody tr:nth-child(odd) td,tbody tr:nth-child(odd) th{background-color:#f9f9f9}.lead{font-size:1.25rem;font-weight:300}.message{margin-bottom:1rem;padding:1rem;color:#717171;background-color:#f9f9f9}.container{max-width:38rem;padding-left:1rem;padding-right:1rem;margin-left:auto;margin-right:auto}.masthead{padding-top:1rem;padding-bottom:1rem;margin-bottom:3rem}.masthead-title{margin-top:0;margin-bottom:0;color:#505050}.masthead-title a{color:#505050}.masthead-title small{font-size:75%;font-weight:400;color:#c0c0c0;letter-spacing:0}.page,.post{margin-bottom:4em}.page-title,.post-title,.post-title a{color:#303030}.page-title,.post-title{margin-top:0}.post-date{display:block;margin-top:-.5rem;margin-bottom:1rem;color:#9a9a9a}.related{padding-top:2rem;padding-bottom:2rem;border-top:1px solid #eee}.related-posts{padding-left:0;list-style:none}.related-posts h3{margin-top:0}.related-posts li small{font-size:75%;color:#999}.related-posts li a:hover{color:#268bd2;text-decoration:none}.related-posts li a:hover small{color:inherit}.pagination{overflow:hidden;margin-left:-1rem;margin-right:-1rem;font-family:"PT Sans", Helvetica, Arial, sans-serif;color:#ccc;text-align:center}.pagination-item{display:block;padding:1rem;border:1px solid #eee}.pagination-item:first-child{margin-bottom:-1px}a.pagination-item:hover{background-color:#f5f5f5}@media (min-width: 30em){.pagination{margin:3rem 0}.pagination-item{float:left;width:50%}.pagination-item:first-child{margin-bottom:0;border-top-left-radius:4px;border-bottom-left-radius:4px}.pagination-item:last-child{margin-left:-1px;border-top-right-radius:4px;border-bottom-right-radius:4px}}html,body{overflow-x:hidden}html{font-family:"PT Serif", Georgia, "Times New Roman", serif}h1,h2,h3,h4,h5,h6{font-family:"PT Sans", Helvetica, Arial, sans-serif;font-weight:400;color:#313131;letter-spacing:-.025rem}.wrap{position:relative;width:100%}.container{max-width:28rem}@media (min-width: 38em){.container{max-width:32rem}}@media (min-width: 56em){.container{max-width:38rem}}.masthead{padding-top:1rem;padding-bottom:1rem;margin-bottom:3rem;border-bottom:1px solid #eee}.masthead-title{margin-top:0;margin-bottom:0;color:#505050}.masthead-title a{color:#505050}.masthead-title small{font-size:75%;font-weight:400;color:#c0c0c0;letter-spacing:0}@media (max-width: 48em){.masthead-title{text-align:center}.masthead-title small{display:none}}.sidebar{position:fixed;top:0;bottom:0;left:-14rem;width:14rem;visibility:hidden;overflow-y:auto;font-family:"PT Sans", Helvetica, Arial, sans-serif;font-size:.875rem;color:rgba(255,255,255,0.6);background-color:#202020;-webkit-transition:all .3s ease-in-out;transition:all .3s ease-in-out}@media (min-width: 30em){.sidebar{font-size:.75rem}}.sidebar a{font-weight:normal;color:#fff}.sidebar-item{padding:1rem}.sidebar-item p:last-child{margin-bottom:0}.sidebar-nav{border-bottom:1px solid rgba(255,255,255,0.1)}.sidebar-nav-item{display:block;padding:.5rem 1rem;border-top:1px solid rgba(255,255,255,0.1)}.sidebar-nav-item.active,a.sidebar-nav-item:hover,a.sidebar-nav-item:focus{text-decoration:none;background-color:rgba(255,255,255,0.1);border-color:transparent}@media (min-width: 48em){.sidebar-item{padding:1.5rem}.sidebar-nav-item{padding-left:1.5rem;padding-right:1.5rem}}.sidebar-checkbox{position:absolute;opacity:0;-webkit-user-select:none;-moz-user-select:none;user-select:none}.sidebar-toggle{position:absolute;top:.8rem;left:1rem;display:flex;align-items:center;padding:.25rem .75rem;color:#505050;background-color:#fff;border-radius:.25rem;cursor:pointer}.sidebar-toggle::before{display:inline-block;width:32px;height:32px;content:"";background:url("data:image/svg+xml,%3Csvg viewBox='0 0 16 16' fill='%23555' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath fill-rule='evenodd' d='M2.5 11.5A.5.5 0 013 11h10a.5.5 0 010 1H3a.5.5 0 01-.5-.5zm0-4A.5.5 0 013 7h10a.5.5 0 010 1H3a.5.5 0 01-.5-.5zm0-4A.5.5 0 013 3h10a.5.5 0 010 1H3a.5.5 0 01-.5-.5z' clip-rule='evenodd'/%3E%3C/svg%3E") no-repeat}.sidebar-toggle:active,#sidebar-checkbox:focus~.sidebar-toggle,#sidebar-checkbox:checked~.sidebar-toggle{color:#fff;background-color:#555}.sidebar-toggle:active:before,#sidebar-checkbox:focus~.sidebar-toggle::before,#sidebar-checkbox:checked~.sidebar-toggle::before{background:url("data:image/svg+xml,%3Csvg viewBox='0 0 16 16' fill='%23fff' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath fill-rule='evenodd' d='M2.5 11.5A.5.5 0 013 11h10a.5.5 0 010 1H3a.5.5 0 01-.5-.5zm0-4A.5.5 0 013 7h10a.5.5 0 010 1H3a.5.5 0 01-.5-.5zm0-4A.5.5 0 013 3h10a.5.5 0 010 1H3a.5.5 0 01-.5-.5z' clip-rule='evenodd'/%3E%3C/svg%3E") no-repeat}@media (min-width: 30.1em){.sidebar-toggle{position:fixed}}@media print{.sidebar-toggle{display:none}}.wrap,.sidebar,.sidebar-toggle{-webkit-backface-visibility:hidden;-ms-backface-visibility:hidden;backface-visibility:hidden}.wrap,.sidebar-toggle{-webkit-transition:-webkit-transform .3s ease-in-out;transition:transform .3s ease-in-out}#sidebar-checkbox:checked+.sidebar{z-index:10;visibility:visible}#sidebar-checkbox:checked~.sidebar,#sidebar-checkbox:checked~.wrap,#sidebar-checkbox:checked~.sidebar-toggle{-webkit-transform:translateX(14rem);-ms-transform:translateX(14rem);transform:translateX(14rem)}.page,.post{margin-bottom:4em}.page-title,.post-title,.post-title a{color:#303030}.page-title,.post-title{margin-top:0}.post-date{display:block;margin-top:-.5rem;margin-bottom:1rem;color:#9a9a9a}.related{padding-top:2rem;padding-bottom:2rem;border-top:1px solid #eee}.related-posts{padding-left:0;list-style:none}.related-posts h3{margin-top:0}.related-posts li small{font-size:75%;color:#999}.related-posts li a:hover{color:#268bd2;text-decoration:none}.related-posts li a:hover small{color:inherit}.pagination{overflow:hidden;margin-left:-1rem;margin-right:-1rem;font-family:"PT Sans", Helvetica, Arial, sans-serif;color:#ccc;text-align:center}.pagination-item{display:block;padding:1rem;border:1px solid #eee}.pagination-item:first-child{margin-bottom:-1px}a.pagination-item:hover{background-color:#f5f5f5}@media (min-width: 30em){.pagination{margin:3rem 0}.pagination-item{float:left;width:50%}.pagination-item:first-child{margin-bottom:0;border-top-left-radius:4px;border-bottom-left-radius:4px}.pagination-item:last-child{margin-left:-1px;border-top-right-radius:4px;border-bottom-right-radius:4px}}.layout-reverse .sidebar{left:auto;right:-14rem}.layout-reverse .sidebar-toggle{left:auto;right:1rem}.layout-reverse #sidebar-checkbox:checked~.sidebar,.layout-reverse #sidebar-checkbox:checked~.wrap,.layout-reverse #sidebar-checkbox:checked~.sidebar-toggle{-webkit-transform:translateX(-14rem);-ms-transform:translateX(-14rem);transform:translateX(-14rem)}.theme-base-08 .sidebar,.theme-base-08 .sidebar-toggle:active,.theme-base-08 #sidebar-checkbox:checked~.sidebar-toggle{background-color:#ac4142}.theme-base-08 .container a,.theme-base-08 .sidebar-toggle,.theme-base-08 .related-posts li a:hover{color:#ac4142}.theme-base-09 .sidebar,.theme-base-09 .sidebar-toggle:active,.theme-base-09 #sidebar-checkbox:checked~.sidebar-toggle{background-color:#d28445}.theme-base-09 .container a,.theme-base-09 .sidebar-toggle,.theme-base-09 .related-posts li a:hover{color:#d28445}.theme-base-0a .sidebar,.theme-base-0a .sidebar-toggle:active,.theme-base-0a #sidebar-checkbox:checked~.sidebar-toggle{background-color:#f4bf75}.theme-base-0a .container a,.theme-base-0a .sidebar-toggle,.theme-base-0a .related-posts li a:hover{color:#f4bf75}.theme-base-0b .sidebar,.theme-base-0b .sidebar-toggle:active,.theme-base-0b #sidebar-checkbox:checked~.sidebar-toggle{background-color:#90a959}.theme-base-0b .container a,.theme-base-0b .sidebar-toggle,.theme-base-0b .related-posts li a:hover{color:#90a959}.theme-base-0c .sidebar,.theme-base-0c .sidebar-toggle:active,.theme-base-0c #sidebar-checkbox:checked~.sidebar-toggle{background-color:#75b5aa}.theme-base-0c .container a,.theme-base-0c .sidebar-toggle,.theme-base-0c .related-posts li a:hover{color:#75b5aa}.theme-base-0d .sidebar,.theme-base-0d .sidebar-toggle:active,.theme-base-0d #sidebar-checkbox:checked~.sidebar-toggle{background-color:#6a9fb5}.theme-base-0d .container a,.theme-base-0d .sidebar-toggle,.theme-base-0d .related-posts li a:hover{color:#6a9fb5}.theme-base-0e .sidebar,.theme-base-0e .sidebar-toggle:active,.theme-base-0e #sidebar-checkbox:checked~.sidebar-toggle{background-color:#aa759f}.theme-base-0e .container a,.theme-base-0e .sidebar-toggle,.theme-base-0e .related-posts li a:hover{color:#aa759f}.theme-base-0f .sidebar,.theme-base-0f .sidebar-toggle:active,.theme-base-0f #sidebar-checkbox:checked~.sidebar-toggle{background-color:#8f5536}.theme-base-0f .container a,.theme-base-0f .sidebar-toggle,.theme-base-0f .related-posts li a:hover{color:#8f5536}.sidebar-overlay #sidebar-checkbox:checked~.wrap{-webkit-transform:translateX(0);-ms-transform:translateX(0);transform:translateX(0)}.sidebar-overlay #sidebar-checkbox:checked~.sidebar-toggle{box-shadow:0 0 0 .25rem #fff}.sidebar-overlay #sidebar-checkbox:checked~.sidebar{box-shadow:0.25rem 0 0.5rem rgba(0,0,0,0.1)}.layout-reverse.sidebar-overlay #sidebar-checkbox:checked~.sidebar{box-shadow:-0.25rem 0 0.5rem rgba(0,0,0,0.1)}.highlight .hll{background-color:#ffc}.highlight .c{color:#999}.highlight .err{color:#a00;background-color:#faa}.highlight .k{color:#069}.highlight .o{color:#555}.highlight .cm{color:#09f;font-style:italic}.highlight .cp{color:#099}.highlight .c1{color:#999}.highlight .cs{color:#999}.highlight .gd{background-color:#fcc;border:1px solid #c00}.highlight .ge{font-style:italic}.highlight .gr{color:#f00}.highlight .gh{color:#030}.highlight .gi{background-color:#cfc;border:1px solid #0c0}.highlight .go{color:#aaa}.highlight .gp{color:#009}.highlight .gu{color:#030}.highlight .gt{color:#9c6}.highlight .kc{color:#069}.highlight .kd{color:#069}.highlight .kn{color:#069}.highlight .kp{color:#069}.highlight .kr{color:#069}.highlight .kt{color:#078}.highlight .m{color:#f60}.highlight .s{color:#d44950}.highlight .na{color:#4f9fcf}.highlight .nb{color:#366}.highlight .nc{color:#0a8}.highlight .no{color:#360}.highlight .nd{color:#99f}.highlight .ni{color:#999}.highlight .ne{color:#c00}.highlight .nf{color:#c0f}.highlight .nl{color:#99f}.highlight .nn{color:#0cf}.highlight .nt{color:#2f6f9f}.highlight .nv{color:#033}.highlight .ow{color:#000}.highlight .w{color:#bbb}.highlight .mf{color:#f60}.highlight .mh{color:#f60}.highlight .mi{color:#f60}.highlight .mo{color:#f60}.highlight .sb{color:#c30}.highlight .sc{color:#c30}.highlight .sd{color:#c30;font-style:italic}.highlight .s2{color:#c30}.highlight .se{color:#c30}.highlight .sh{color:#c30}.highlight .si{color:#a00}.highlight .sx{color:#c30}.highlight .sr{color:#3aa}.highlight .s1{color:#c30}.highlight .ss{color:#fc3}.highlight .bp{color:#366}.highlight .vc{color:#033}.highlight .vg{color:#033}.highlight .vi{color:#033}.highlight .il{color:#f60}.css .o,.css .o+.nt,.css .nt+.nt{color:#999}</style><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400&display=swap"><link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/public/apple-touch-icon-precomposed.png"><link rel="shortcut icon" href="http://localhost:4000/public/favicon.ico"><link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/atom.xml"><body> <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox"><div class="sidebar" id="sidebar"><div class="sidebar-item"><p>A blog on machine learning, probabilistic methods and intelligent systems.</p></div><nav class="sidebar-nav"> <a class="sidebar-nav-item" href="http://localhost:4000/">Home</a> <a class="sidebar-nav-item" href="http://localhost:4000/about/">About</a></nav><div class="sidebar-item"><p> &copy; 2021. All rights reserved.</p></div></div><div class="wrap"><div class="masthead"><div class="container"><h3 class="masthead-title"> <a href="/" title="Home">tensor.blog</a> <small>thinking machines</small></h3></div></div><div class="container content"><div class="post"><h1 class="post-title">Talking to myself using GPT</h1><span class="post-date">14 Jul 2021</span><p>Transformer &amp; attention models are all the hype in the machine learning community recently, so I took sometime to learn about them. As a fun project, I’ve decided to build a virtual version of myself (now I will finally have a friend to talk to!).</p><h2 id="data-preparation">Data preparation</h2><p>The training data was prepared through <a href="https://www.facebook.com/dyi/">downloading my entire Facebook chat history</a>, and using a <a href="https://github.com/agiantwhale/jaegpt/blob/master/build_dataset.py">parsing script</a> I wrote. There’s a couple of gotchas when it comes to parsing Facebook messages data:</p><ul><li>Ignore chat threads where most of the language is non-English (Korean is my first language).<li>Remove automated messages (A lot of <em>Words With Friends</em> requests).<li>Combine consecutive messages into a single sentence.<li>Set a threshold to yield a new context (I’ve chosen this as 50th percentile of message time delta).</ul><p>Overall, the preparation pipeline looks as follows: <img src="/assets/jaegpt/data_gen.png" alt="GPT Data Generation Pipeline" /></p><p>The context of the model is preceding 4 messages before my reply. 3 distractors are chosen from the sent replies in the current chat for the supervision task. I haven’t investigated the effects of these two parameter choices – tuning these context and distractor parameters could be an interesting study.</p><p>The final dataset yields 29192 lines and is 11MB. I’ve split as 25000 lines as training set and the rest 4192 lines as the validation set. This is a fraction of the amount of data the baseline model was trained on (<a href="https://github.com/microsoft/DialoGPT">DialoGPT was trained on 147M multi-turn dialogue from Reddit</a>). Regardless, I found that I was able to get reasonable results with these small datasets.</p><p>Here is the final objective distribution that we aim to model.</p><p><img class="center" src="/latex/latex-1130b28c21c2a95b5536f6c7eb3bc1f6.png" /></p><h2 id="model-architecture">Model architecture</h2><p>The architecture is taken directly from the <a href="https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313">HuggingFace’s ConvAI2 (NeurIPS 2018) winning model</a>. The language modeling head is making the actual token predictions, with an extra next sentence classification head added on to the hidden states of the last token to discriminate between the correct reply and a negatively sampled reply. Refer to the linked blogpost and the code for details – the transformer network (and attention module) is a fascinating piece of work that deserves close… <em>attention</em>. For the sake of this post, you can think of the attention module as learning a masking function that focuses on each word in a given sentence (in a <em>fill-in-the-blank</em> manner).</p><p><img src="/assets/jaegpt/model_design.png" alt="GPT2 Double Head Model Design" /></p><p>It seems the discriminator head is designed to act as a weak supervision function to aid language modeling task (through cross learning), but <strong>I didn’t notice any noticable improvements to the main LM task without it.</strong> More details in the following sections.</p><h2 id="training-implementation">Training implementation</h2><p>I’m relying on HuggingFace’s Transformer library to train the model on Google Colab’s TPUs. I ran into several problems, which I mostly fixed by copy-paste engineering and customizing to suit my needs:</p><ul><li>HuggingFace’s Trainer (as of v4.8.2) does not support exporting multiple losses. The final optimization loss is the weighted sum of two tasks: <img class="inline" src="/latex/latex-367d42f5b8fcd77a92884b4b71726667.png" />, but for final evaluation we only care about the language modeling task (<img class="inline" src="/latex/latex-6c9c382af2e89e83ff13a592ab81e473.png" />). A quick hack to report all three loss is in the code implementation <a href="https://github.com/agiantwhale/jaegpt/blob/master/fbgpt/trainer.py">here</a>.<li>Training on TPU (using <a href="https://github.com/pytorch/xla">pytorch/xla</a>) requires fixed tensor sizes for speed up. This requires some additional care on preprocessing the data part (see block size optimization on parameter tuning section below). I assume this is due to TPU internally optimizing the memory layout.<li>DialoGPT was trained without using token IDs.</ul><p>The following sections describe the parameter tuning explorations.</p><h3 id="block-size">Block size</h3><p>This is to resolve the fixed tensor requirement with the TPU above. I wrote a script to calculate percentage of training rows that go above limit at each tokenization step. From the chart below, it seems we get a 99% yield with block size of <img class="inline" src="/latex/latex-e0afb0c7a5e6c6ec867e9bbea08b6b10.png" />.</p><p><img src="/assets/jaegpt/block_size.png" alt="GPT2 Double Head Model Design" /></p><h3 id="epoch">Epoch</h3><p>I kicked off an exploratory run with epoch of 4 to see if multi-epoch training is even reasonable. Small dataset, large model size and sparse tokens seems like a recipe for overfitting, so I chose a batch size of 1 TPU (which gives us an update batch size of 8, as there are 8 TPU cores). Next sentence prediction task was not used in this run.</p><p><img src="/assets/jaegpt/epoch_tune.png" alt="Train vs Eval in Epoch 4" /></p><p>We seem to overfit after 1 epoch. The other runs were trained using a single epoch.</p><h3 id="task-weight-exploration">Task weight exploration</h3><p>For a baseline, below is the entropy of a background predictors that generate a random choice for the two tasks. The DialoGPT tokenizer contains 50257 tokens, and we choose 3 distractors for the next sentence prediction tasks.</p><p><img class="center" src="/latex/latex-7b1b14389b8ed1593e58412ea7b2d214.png" /></p><p>The background entropy for language modeling task is 7.8 times higher than next sentence prediction! A 50% reduction in the classification task would be considered only a 7% reduction in the language modeling task in the final loss function.</p><p>To verify the potential improvements, I tuned the <img class="inline" src="/latex/latex-c08a7b5eb987e4198f55df3047c97bf7.png" /> in the combined loss to the following values. Reported metrics are eval metrics.</p><table><thead><tr><th><img class="inline" src="/latex/latex-c08a7b5eb987e4198f55df3047c97bf7.png" /><th><img class="inline" src="/latex/latex-480ff5082f729e253113a042a6c9bbac.png" /><th><img class="inline" src="/latex/latex-810c648c5a616153702c17cba3428550.png" /><th><img class="inline" src="/latex/latex-6c9c382af2e89e83ff13a592ab81e473.png" /><th><img class="inline" src="/latex/latex-18fb601c4ccd970049908cca46af8ff2.png" /><tbody><tr><td>100<td>1.819<td>1.312<td>7.814<td>0.722<tr><td>10<td>1.328<td>0.958<td>5.229<td>0.483<tr><td>1<td><strong>1.063</strong><td><strong>0.767</strong><td>4.503<td>0.415<tr><td>0.1<td>1.325<td>0.956<td>4.365<td>0.403<tr><td>0.01<td>1.386<td>0.999<td><strong>4.361</strong><td><strong>0.402</strong></table><p>And their corresponding learning curves:</p><p><img src="/assets/jaegpt/lm_loss.png" alt="Eval LM Loss" /></p><p>Using the supervision tasks shows minimal improvement to the main LM loss; rather makes it worse.</p><p><img src="/assets/jaegpt/acc.png" alt="Eval Accuracy" /></p><p>Surprisingly, the inclusion of supervision tasks increase the LM modeling accuracy. However this isn’t a good metric to optimize for, as we will be using beam search to generate responses through sampling; learning an accurate distribution of next token is more important than predicting the most likely token.</p><p><img src="/assets/jaegpt/mc_loss.png" alt="Eval MC Loss" /></p><p>Increasing the <img class="inline" src="/latex/latex-c08a7b5eb987e4198f55df3047c97bf7.png" /> parameter does not result in linear increase to MC loss, with <img class="inline" src="/latex/latex-f6fbad6a2333abe8d09954cff4137205.png" /> being the optimal state. I couldn’t think of a reasonable explanation for this; my guess is it has to do with steep gradient magnitudes with higher <img class="inline" src="/latex/latex-c08a7b5eb987e4198f55df3047c97bf7.png" />; plotting magnitude of gradient might give us a better idea here.</p><h3 id="bayes-hyperparameter-optimization">Bayes hyperparameter optimization</h3><p>We did some initial exploration of model architectures; now let’s try optimize the model to get the best candidate.</p><h2 id="conversation-examples">Conversation examples</h2></div><div class="related"><h2>Related posts</h2><ul class="related-posts"><li><h3> <a href="/2020/08/03/group-lasso/"> Intuitive Explanation of Group LASSO Regularization for Neural Network Interpretability <small>03 Aug 2020</small> </a></h3></ul></div></div></div><label for="sidebar-checkbox" class="sidebar-toggle"></label> <script> (function(document) { var toggle = document.querySelector('.sidebar-toggle'); var sidebar = document.querySelector('#sidebar'); var checkbox = document.querySelector('#sidebar-checkbox'); document.addEventListener('click', function(e) { var target = e.target; if(!checkbox.checked || sidebar.contains(target) || (target === checkbox || target === toggle)) return; checkbox.checked = false; }, false); })(document); </script>
