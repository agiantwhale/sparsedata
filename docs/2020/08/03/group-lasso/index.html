<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Intuitive Explanation of Group LASSO Regularization for Neural Network Interpretability &middot; SPARSEDATA.ML
    
  </title>

  
  <link rel="canonical" href="https://sparsedata.ml/2020/08/03/group-lasso/">
  

  <link rel="stylesheet" href="https://sparsedata.ml/public/css/poole.css">
  <link rel="stylesheet" href="https://sparsedata.ml/public/css/syntax.css">
  <link rel="stylesheet" href="https://sparsedata.ml/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400&display=swap">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://sparsedata.ml/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="https://sparsedata.ml/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://sparsedata.ml/atom.xml">

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>On machine learning & statistics</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://sparsedata.ml/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://sparsedata.ml/about/">About</a>
        
      
    
      
    
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2020. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">SPARSEDATA.ML</a>
            <small>thinking machines</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Intuitive Explanation of Group LASSO Regularization for Neural Network Interpretability</h1>
  <span class="post-date">03 Aug 2020</span>
  <p>Neural networks are often referred to as a black box model because of its lack of interpretability. Most of a network’s operations occur in the hidden layers and latent space. As a result, tracing important features in a dataset is not an easy task, especially when the number of features is large. This is often a limiting factor in applying neural networks in fields where explainability is not only favored, but crucial (such as medical diagnostics or finance).</p>

<p>Through this entry, we hope to examine the application of the group LASSO regularization for solving the problems described above.</p>

<h2 id="what-is-ridge-and-lasso-regularization">What is ridge and LASSO regularization?</h2>
<p>The loss function of ridge regression can be defined as</p>

<p><img class="center" src="/latex/latex-b832c44e16abdcd71034eb8a69f73d4f.png" /></p>

<p>while loss function of LASSO regression can be defined as</p>

<p><img class="center" src="/latex/latex-c706b7933e2425517886943a7c4855dd.png" /></p>

<p>The above loss functions can be broken down into</p>
<ul>
  <li>Predicted output: <img class="inline" src="/latex/latex-1f625f17552856d6463fcac3c32101b3.png" /></li>
  <li>Regularization term
    <ul>
      <li><img class="inline" src="/latex/latex-c343adb0c473016c19f48b35e9ee1923.png" /> for LASSO</li>
      <li><img class="inline" src="/latex/latex-fa17a9b380844ff87dc1095a6f843f17.png" /> for ridge</li>
    </ul>
  </li>
</ul>

<p>Comparison of the two regularization terms shows the intuition behind LASSO regression’s better interpretability characteristics. From a Big-O standpoint, <img class="inline" src="/latex/latex-f037b2a8bf7aa27b03b3a16f052acd06.png" />. The penalty for having one skewed large value is much greater for ridge regression. Ridge regularization aims to reduce variance between the coefficients, therefore driving all features down to zero.</p>

<p>LASSO regularization, on the other hand, will set some feature’s coefficients to zero values when deemed necessary, effectively removing them. We then can compare non-zero coefficients to determine the importance of the features.</p>

<h2 id="what-is-group-lasso-regularization">What is group LASSO regularization?</h2>
<p>From the above example, we observe how LASSO regularization can help with the interpretability of the model. But some problems may benefit from a group of features used together, especially when incorporating domain knowledge into the model.</p>

<p>Group LASSO attempts to solve this problem by separating the entire feature set into separate feature groups. The regularization function can be written as</p>

<p><img class="center" src="/latex/latex-982f81b039618d09108f899cf26f46c5.png" /></p>

<p>where</p>
<ul>
  <li><img class="inline" src="/latex/latex-10afdb7058d58d26e17462afab5184e1.png" /> denotes the size of the group.</li>
  <li><img class="inline" src="/latex/latex-1260e71e0fc41670c2fe966156d4aa01.png" /> denotes the L2-norm of the feature group <img class="inline" src="/latex/latex-4322e011bfdf0bfdc4c27891ff3dfc61.png" />.</li>
</ul>

<p>Let’s take a closer look at the regularization term <img class="inline" src="/latex/latex-efcb5f9f42d3fc2659fff45d27825d09.png" />.</p>

<p>Note that <img class="inline" src="/latex/latex-04f7d616922df5bee90be4315df5bd74.png" />, and we for some <img class="inline" src="/latex/latex-3fc4979464ed4d76e333e61e3bd92615.png" /> that satisfies <img class="inline" src="/latex/latex-d9cbfed9c4e18f1438b295b5ed388b2c.png" />, we could effectively rewrite the equation as</p>

<p><img class="center" src="/latex/latex-502db40ff96a3f8501db90bbd383a2f1.png" /></p>

<p>In this case, we have effectively reduced the regularization to LASSO regularization on the inter-group level.</p>

<p>Similarly, let’s take a look an subgroup. Expanding the term for some group with cardinality <img class="inline" src="/latex/latex-cf56f66cf006f2f909a26e2840fb3197.png" />, the regularization term can be expressed as</p>

<p><img class="center" src="/latex/latex-ef2df46f53bb8711bf2f93489981cbfc.png" /></p>

<p>Here, we have effectively reduced the regularization to ridge regularization on the intra-group level.</p>

<p>We build on the intuition that while it cannot select certain features within the same group, because of it’s LASSO-like nature between feature groups, the model will zero-out entirety of certain coefficient groups.</p>

<p>Additionally, note the two following characteristics:</p>
<ul>
  <li>When <img class="inline" src="/latex/latex-b509903172b539c1855c10cb08e2b0a9.png" />, the regularization term essentially becomes a LASSO (L1) regularization.</li>
  <li>When <img class="inline" src="/latex/latex-67dc231cbb67be814d1770bc40e27f66.png" />, the regularization term essentially becomes a ridge (L2) regularization.</li>
</ul>

<h2 id="how-can-we-adapt-group-lasso-for-neural-networks">How can we adapt group LASSO for neural networks?</h2>
<p>Up to now, the application of regularization terms have been on linear regression methods where each features are assigned a single coefficient weight. Now, we will take a look at a neural network, specifically on the connections between the first two layer of the network, where each individual features have multiple weights associated to the next layer.</p>

<p>To visualize this, say we have a small neural network with one hidden layer.</p>

<p><img class="center" src="/latex/latex-676b7e3e942012b08f4761cb52d6598c.png" /></p>

<p>In order for the above feature selection to work, we will need to zero out the weights connected for all of feature <img class="inline" src="/latex/latex-63ac1428cb664318e538521970b9c31d.png" /> (marked in red).</p>

<p><img class="center" src="/latex/latex-6380075deae1b69a4865b8777a069e99.png" /></p>

<p>In this case, the weights associated with each of the neurons becomes becomes a group of their own. Let <img class="inline" src="/latex/latex-588e7585b44191d96d5d21a3b0f9d8af.png" /> and <img class="inline" src="/latex/latex-f78c1d7bf8cdc1fa8475ee763ebaa13f.png" /> denote the weight vectors for input features <img class="inline" src="/latex/latex-a61fcbe9f079dadb08038189261d695e.png" /> and <img class="inline" src="/latex/latex-63ac1428cb664318e538521970b9c31d.png" /> (<img class="inline" src="/latex/latex-f78c1d7bf8cdc1fa8475ee763ebaa13f.png" /> weights would be marked in red above). We can adapt the group LASSO regularization formulation as</p>

<p><img class="center" src="/latex/latex-ef01b74a4d23c0555bc00ca85c7fcbaa.png" /></p>

<p>where <img class="inline" src="/latex/latex-ad4c0ceaad058aef6ccf3e724ca12b9a.png" /> denotes the loss function and <img class="inline" src="/latex/latex-3035883f6531462455e562d330180152.png" /> denotes the full-connected weights to feature <img class="inline" src="/latex/latex-7bdce1f20016bc5e7197ccbbb0a86fab.png" />. Since we have two input features, the regularization term would also expand to</p>

<p><img class="center" src="/latex/latex-987cc3e980a19372984ddcd977ea3a9a.png" /></p>

<p>We have essentially derived the Group level lasso regularization on each of the individual features, with the weights corresponding to each feature in a group. We can continue to build on the intuition from the Group LASSO.</p>

<p>While each individual weights inside a weight group will not differ in terms of convergence to zero (all elements of <img class="inline" src="/latex/latex-588e7585b44191d96d5d21a3b0f9d8af.png" />, <img class="inline" src="/latex/latex-f78c1d7bf8cdc1fa8475ee763ebaa13f.png" /> will either be zero or non-zero), the non-continuous nature of the l2 norm for individual features will introduce sparsity and converge entire feature weights to 0.</p>

<p>From here, it’s trivial to apply the same technique to regularizing hidden layers to introduce further sparsity to the model and improve model capacity or prune unneeded connections.</p>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://icml.cc/Conferences/2010/papers/473.pdf">Yang, Haiqin et al. “Online Learning for Group Lasso.” <em>ICML</em> (2010).</a></li>
  <li><a href="https://arxiv.org/abs/1607.00485">Scardapane, Simone et al. “Group Sparse Regularization for Deep Neural Networks.” Neurocomputing 241 (2017): 81–89. Crossref. Web.</a></li>
</ul>

</div>



      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
