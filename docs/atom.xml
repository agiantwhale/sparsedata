<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>SPARSEDATA.ML</title>
 <link href="https://sparsedata.ml/atom.xml" rel="self"/>
 <link href="https://sparsedata.ml/"/>
 <updated>2020-09-03T16:45:33-07:00</updated>
 <id>https://sparsedata.ml</id>
 <author>
   <name>Il Jae Lee</name>
   <email>agiantwhale@gmail.com</email>
 </author>

 
 <entry>
   <title>Intuitive Explanation of Group LASSO Regularization for Neural Network Interpretability</title>
   <link href="https://sparsedata.ml/2020/08/03/group-lasso/"/>
   <updated>2020-08-03T00:00:00-07:00</updated>
   <id>https://sparsedata.ml/2020/08/03/group-lasso</id>
   <content type="html">&lt;p&gt;Neural networks are often referred to as a black box model because of its lack of interpretability. Most of a network’s operations occur in the hidden layers and latent space. As a result, tracing important features in a dataset is not an easy task, especially when the number of features is large. This is often a limiting factor in applying neural networks in fields where explainability is not only favored, but crucial (such as medical diagnostics or finance).&lt;/p&gt;

&lt;p&gt;Through this entry, we hope to examine the application of the group LASSO regularization for solving the problems described above.&lt;/p&gt;

&lt;h2 id=&quot;what-is-ridge-and-lasso-regularization&quot;&gt;What is ridge and LASSO regularization?&lt;/h2&gt;
&lt;p&gt;The loss function of ridge regression can be defined as&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/latex/latex-b832c44e16abdcd71034eb8a69f73d4f.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;while loss function of LASSO regression can be defined as&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/latex/latex-c706b7933e2425517886943a7c4855dd.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above loss functions can be broken down into&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Predicted output: &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-1f625f17552856d6463fcac3c32101b3.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Regularization term
    &lt;ul&gt;
      &lt;li&gt;&lt;img class=&quot;inline&quot; src=&quot;/latex/latex-c343adb0c473016c19f48b35e9ee1923.png&quot; /&gt; for LASSO&lt;/li&gt;
      &lt;li&gt;&lt;img class=&quot;inline&quot; src=&quot;/latex/latex-fa17a9b380844ff87dc1095a6f843f17.png&quot; /&gt; for ridge&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Comparison of the two regularization terms shows the intuition behind LASSO regression’s better interpretability characteristics. From a Big-O standpoint, &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-f037b2a8bf7aa27b03b3a16f052acd06.png&quot; /&gt;. The penalty for having one skewed large value is much greater for ridge regression. Ridge regularization aims to reduce variance between the coefficients, therefore driving all features down to zero.&lt;/p&gt;

&lt;p&gt;LASSO regularization, on the other hand, will set some feature’s coefficients to zero values when deemed necessary, effectively removing them. We then can compare non-zero coefficients to determine the importance of the features.&lt;/p&gt;

&lt;h2 id=&quot;what-is-group-lasso-regularization&quot;&gt;What is group LASSO regularization?&lt;/h2&gt;
&lt;p&gt;From the above example, we observe how LASSO regularization can help with the interpretability of the model. But some problems may benefit from a group of features used together, especially when incorporating domain knowledge into the model.&lt;/p&gt;

&lt;p&gt;Group LASSO attempts to solve this problem by separating the entire feature set into separate feature groups. The regularization function can be written as&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/latex/latex-982f81b039618d09108f899cf26f46c5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;img class=&quot;inline&quot; src=&quot;/latex/latex-10afdb7058d58d26e17462afab5184e1.png&quot; /&gt; denotes the size of the group.&lt;/li&gt;
  &lt;li&gt;&lt;img class=&quot;inline&quot; src=&quot;/latex/latex-1260e71e0fc41670c2fe966156d4aa01.png&quot; /&gt; denotes the L2-norm of the feature group &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-4322e011bfdf0bfdc4c27891ff3dfc61.png&quot; /&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s take a closer look at the regularization term &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-efcb5f9f42d3fc2659fff45d27825d09.png&quot; /&gt;.&lt;/p&gt;

&lt;p&gt;Note that &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-04f7d616922df5bee90be4315df5bd74.png&quot; /&gt;, and we for some &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-3fc4979464ed4d76e333e61e3bd92615.png&quot; /&gt; that satisfies &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-d9cbfed9c4e18f1438b295b5ed388b2c.png&quot; /&gt;, we could effectively rewrite the equation as&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/latex/latex-502db40ff96a3f8501db90bbd383a2f1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, we have effectively reduced the regularization to LASSO regularization on the inter-group level.&lt;/p&gt;

&lt;p&gt;Similarly, let’s take a look an subgroup. Expanding the term for some group with cardinality &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-cf56f66cf006f2f909a26e2840fb3197.png&quot; /&gt;, the regularization term can be expressed as&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/latex/latex-ef2df46f53bb8711bf2f93489981cbfc.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, we have effectively reduced the regularization to ridge regularization on the intra-group level.&lt;/p&gt;

&lt;p&gt;We build on the intuition that while it cannot select certain features within the same group, because of it’s LASSO-like nature between feature groups, the model will zero-out entirety of certain coefficient groups.&lt;/p&gt;

&lt;p&gt;Additionally, note the two following characteristics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;When &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-b509903172b539c1855c10cb08e2b0a9.png&quot; /&gt;, the regularization term essentially becomes a LASSO (L1) regularization.&lt;/li&gt;
  &lt;li&gt;When &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-67dc231cbb67be814d1770bc40e27f66.png&quot; /&gt;, the regularization term essentially becomes a ridge (L2) regularization.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-can-we-adapt-group-lasso-for-neural-networks&quot;&gt;How can we adapt group LASSO for neural networks?&lt;/h2&gt;
&lt;p&gt;Up to now, the application of regularization terms have been on linear regression methods where each features are assigned a single coefficient weight. Now, we will take a look at a neural network, specifically on the connections between the first two layer of the network, where each individual features have multiple weights associated to the next layer.&lt;/p&gt;

&lt;p&gt;To visualize this, say we have a small neural network with one hidden layer.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/latex/latex-676b7e3e942012b08f4761cb52d6598c.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In order for the above feature selection to work, we will need to zero out the weights connected for all of feature &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-63ac1428cb664318e538521970b9c31d.png&quot; /&gt; (marked in red).&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/latex/latex-6380075deae1b69a4865b8777a069e99.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, the weights associated with each of the neurons becomes becomes a group of their own. Let &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-588e7585b44191d96d5d21a3b0f9d8af.png&quot; /&gt; and &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-f78c1d7bf8cdc1fa8475ee763ebaa13f.png&quot; /&gt; denote the weight vectors for input features &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-a61fcbe9f079dadb08038189261d695e.png&quot; /&gt; and &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-63ac1428cb664318e538521970b9c31d.png&quot; /&gt; (&lt;img class=&quot;inline&quot; src=&quot;/latex/latex-f78c1d7bf8cdc1fa8475ee763ebaa13f.png&quot; /&gt; weights would be marked in red above). We can adapt the group LASSO regularization formulation as&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/latex/latex-ef01b74a4d23c0555bc00ca85c7fcbaa.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-ad4c0ceaad058aef6ccf3e724ca12b9a.png&quot; /&gt; denotes the loss function and &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-3035883f6531462455e562d330180152.png&quot; /&gt; denotes the full-connected weights to feature &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-7bdce1f20016bc5e7197ccbbb0a86fab.png&quot; /&gt;. Since we have two input features, the regularization term would also expand to&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/latex/latex-987cc3e980a19372984ddcd977ea3a9a.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We have essentially derived the Group level lasso regularization on each of the individual features, with the weights corresponding to each feature in a group. We can continue to build on the intuition from the Group LASSO.&lt;/p&gt;

&lt;p&gt;While each individual weights inside a weight group will not differ in terms of convergence to zero (all elements of &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-588e7585b44191d96d5d21a3b0f9d8af.png&quot; /&gt;, &lt;img class=&quot;inline&quot; src=&quot;/latex/latex-f78c1d7bf8cdc1fa8475ee763ebaa13f.png&quot; /&gt; will either be zero or non-zero), the non-continuous nature of the l2 norm for individual features will introduce sparsity and converge entire feature weights to 0.&lt;/p&gt;

&lt;p&gt;From here, it’s trivial to apply the same technique to regularizing hidden layers to introduce further sparsity to the model and improve model capacity or prune unneeded connections.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://icml.cc/Conferences/2010/papers/473.pdf&quot;&gt;Yang, Haiqin et al. “Online Learning for Group Lasso.” &lt;em&gt;ICML&lt;/em&gt; (2010).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1607.00485&quot;&gt;Scardapane, Simone et al. “Group Sparse Regularization for Deep Neural Networks.” Neurocomputing 241 (2017): 81–89. Crossref. Web.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 

</feed>
